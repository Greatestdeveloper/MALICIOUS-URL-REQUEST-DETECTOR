{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b647c3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Phishing url detection\n"
     ]
    }
   ],
   "source": [
    "cd D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Phishing url detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec6b10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\lanka\\anaconda3\\lib\\site-packages (4.6.0)\n",
      "Requirement already satisfied: urllib3[socks]~=1.26 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from selenium) (1.26.13)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from selenium) (0.9.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from selenium) (2021.10.8)\n",
      "Requirement already satisfied: trio~=0.17 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from selenium) (0.22.0)\n",
      "Requirement already satisfied: sniffio in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.2.0 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (21.4.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.0rc9 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.0.4)\n",
      "Requirement already satisfied: cffi>=1.14 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.15.0)\n",
      "Requirement already satisfied: outcome in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (2.5)\n",
      "Requirement already satisfied: async-generator>=1.9 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio~=0.17->selenium) (1.10)\n",
      "Requirement already satisfied: pycparser in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from cffi>=1.14->trio~=0.17->selenium) (2.21)\n",
      "Requirement already satisfied: wsproto>=0.14 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\lanka\\anaconda3\\lib\\site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# import seaborn as sb # helps in statistical data visualization in different forms\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "\n",
    "import matplotlib.pyplot as plt # plots the data\n",
    "%matplotlib inline \n",
    "\n",
    "import time \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.naive_bayes import MultinomialNB # a simple learning algo which used bayes rule assuming the objects having some characteristics \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer # Tokenizes the url to get the required words\n",
    "from nltk.stem.snowball import SnowballStemmer # stemms different works into meaningless words \n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer # Vectorizes the tokens into a matrix \n",
    "from sklearn.pipeline import make_pipeline \n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "!pip install selenium \n",
    "from selenium import webdriver\n",
    "import networkx as nx # plots the internal link redirections within a website\n",
    "\n",
    "import pickle # we'll store data in form of non-primitives and that has to be converted to character stream when we train it and \"pickle\" helps us to do that \n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80424da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "phish_data = {}\n",
    "phish_data = pd.read_csv('phishing_site_urls.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5bba844f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phish_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "588c7102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "URL      0\n",
       "Label    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b6005f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>good</th>\n",
       "      <td>392924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bad</th>\n",
       "      <td>156422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label\n",
       "good  392924\n",
       "bad   156422"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_counts = pd.DataFrame(phish_data.Label.value_counts())\n",
    "label_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873c49de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_style('darkgrid')\n",
    "sb.barplot(label_counts.index,label_counts.Label)\n",
    "# sb.barplot( index , labels ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d538e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "    tokenizer = RegexpTokenizer(r'[A-Za-z]+') # tokenizes the given url based on the string argument r'[a-zA-Z]\\w+\\'?\\w*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9367bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizing all the rows\n",
    "# by passing all the URLs into the anonymous function i.e. mapping all the URLs into anonymous function.\n",
    "# we use anonymous functions here for simplicity of code and no requirement of creating a function and storing it.\n",
    "phish_data['text_tokenized'] = phish_data.URL.map(lambda t: tokenizer.tokenize(t)) # doing with all rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "512af4d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>391655</th>\n",
       "      <td>montrealinc.ca/en/index.php</td>\n",
       "      <td>good</td>\n",
       "      <td>[montrealinc, ca, en, index, php]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>419610</th>\n",
       "      <td>raiders.com/schedule/season-schedule.html</td>\n",
       "      <td>good</td>\n",
       "      <td>[raiders, com, schedule, season, schedule, html]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228758</th>\n",
       "      <td>pipl.com/directory/name/Guyon/Peter</td>\n",
       "      <td>good</td>\n",
       "      <td>[pipl, com, directory, name, Guyon, Peter]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148742</th>\n",
       "      <td>bentley.umich.edu/exhibits/umtimeline/general.php</td>\n",
       "      <td>good</td>\n",
       "      <td>[bentley, umich, edu, exhibits, umtimeline, ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326243</th>\n",
       "      <td>facebook.com/ShawneeTownship</td>\n",
       "      <td>good</td>\n",
       "      <td>[facebook, com, ShawneeTownship]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      URL Label  \\\n",
       "391655                        montrealinc.ca/en/index.php  good   \n",
       "419610          raiders.com/schedule/season-schedule.html  good   \n",
       "228758                pipl.com/directory/name/Guyon/Peter  good   \n",
       "148742  bentley.umich.edu/exhibits/umtimeline/general.php  good   \n",
       "326243                       facebook.com/ShawneeTownship  good   \n",
       "\n",
       "                                           text_tokenized  \n",
       "391655                  [montrealinc, ca, en, index, php]  \n",
       "419610   [raiders, com, schedule, season, schedule, html]  \n",
       "228758         [pipl, com, directory, name, Guyon, Peter]  \n",
       "148742  [bentley, umich, edu, exhibits, umtimeline, ge...  \n",
       "326243                   [facebook, com, ShawneeTownship]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_data.sample(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2de4ead",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bac86997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stemming all the repetetive words into meaning less words\n",
    "phish_data['text_stemmed'] = phish_data['text_tokenized'].map(lambda l: [stemmer.stem(word) for word in l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "87c6aacb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[nobell, it, ffb, d, dca, cce, f, login, SkyPe...</td>\n",
       "      <td>[nobel, it, ffb, d, dca, cce, f, login, skype,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>bad</td>\n",
       "      <td>[serviciosbys, com, paypal, cgi, bin, get, int...</td>\n",
       "      <td>[serviciosbi, com, paypal, cgi, bin, get, into...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>bad</td>\n",
       "      <td>[mail, printakid, com, www, online, americanex...</td>\n",
       "      <td>[mail, printakid, com, www, onlin, americanexp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[thewhiskeydregs, com, wp, content, themes, wi...</td>\n",
       "      <td>[thewhiskeydreg, com, wp, content, theme, wide...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smilesvoegol.servebbs.org/voegol.php</td>\n",
       "      <td>bad</td>\n",
       "      <td>[smilesvoegol, servebbs, org, voegol, php]</td>\n",
       "      <td>[smilesvoegol, servebb, org, voegol, php]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>premierpaymentprocessing.com/includes/boleto-2...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[premierpaymentprocessing, com, includes, bole...</td>\n",
       "      <td>[premierpaymentprocess, com, includ, boleto, v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>myxxxcollection.com/v1/js/jih321/bpd.com.do/do...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[myxxxcollection, com, v, js, jih, bpd, com, d...</td>\n",
       "      <td>[myxxxcollect, com, v, js, jih, bpd, com, do, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>super1000.info/docs</td>\n",
       "      <td>bad</td>\n",
       "      <td>[super, info, docs]</td>\n",
       "      <td>[super, info, doc]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>horizonsgallery.com/js/bin/ssl1/_id/www.paypal...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[horizonsgallery, com, js, bin, ssl, id, www, ...</td>\n",
       "      <td>[horizonsgalleri, com, js, bin, ssl, id, www, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phlebolog.com.ua/libraries/joomla/results.php</td>\n",
       "      <td>bad</td>\n",
       "      <td>[phlebolog, com, ua, libraries, joomla, result...</td>\n",
       "      <td>[phlebolog, com, ua, librari, joomla, result, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>docs.google.com/spreadsheet/viewform?formkey=d...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[docs, google, com, spreadsheet, viewform, for...</td>\n",
       "      <td>[doc, googl, com, spreadsheet, viewform, formk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>www.coincoele.com.br/Scripts/smiles/?pt-br/Pag...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, coincoele, com, br, Scripts, smiles, pt,...</td>\n",
       "      <td>[www, coincoel, com, br, script, smile, pt, br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>www.henkdeinumboomkwekerij.nl/language/pdf_fon...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, henkdeinumboomkwekerij, nl, language, pd...</td>\n",
       "      <td>[www, henkdeinumboomkwekerij, nl, languag, pdf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>perfectsolutionofall.net/wp-content/themes/twe...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[perfectsolutionofall, net, wp, content, theme...</td>\n",
       "      <td>[perfectsolutionofal, net, wp, content, theme,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL Label  \\\n",
       "0   nobell.it/70ffb52d079109dca5664cce6f317373782/...   bad   \n",
       "1   www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...   bad   \n",
       "2   serviciosbys.com/paypal.cgi.bin.get-into.herf....   bad   \n",
       "3   mail.printakid.com/www.online.americanexpress....   bad   \n",
       "4   thewhiskeydregs.com/wp-content/themes/widescre...   bad   \n",
       "5                smilesvoegol.servebbs.org/voegol.php   bad   \n",
       "6   premierpaymentprocessing.com/includes/boleto-2...   bad   \n",
       "7   myxxxcollection.com/v1/js/jih321/bpd.com.do/do...   bad   \n",
       "8                                 super1000.info/docs   bad   \n",
       "9   horizonsgallery.com/js/bin/ssl1/_id/www.paypal...   bad   \n",
       "10      phlebolog.com.ua/libraries/joomla/results.php   bad   \n",
       "11  docs.google.com/spreadsheet/viewform?formkey=d...   bad   \n",
       "12  www.coincoele.com.br/Scripts/smiles/?pt-br/Pag...   bad   \n",
       "13  www.henkdeinumboomkwekerij.nl/language/pdf_fon...   bad   \n",
       "14  perfectsolutionofall.net/wp-content/themes/twe...   bad   \n",
       "\n",
       "                                       text_tokenized  \\\n",
       "0   [nobell, it, ffb, d, dca, cce, f, login, SkyPe...   \n",
       "1   [www, dghjdgf, com, paypal, co, uk, cycgi, bin...   \n",
       "2   [serviciosbys, com, paypal, cgi, bin, get, int...   \n",
       "3   [mail, printakid, com, www, online, americanex...   \n",
       "4   [thewhiskeydregs, com, wp, content, themes, wi...   \n",
       "5          [smilesvoegol, servebbs, org, voegol, php]   \n",
       "6   [premierpaymentprocessing, com, includes, bole...   \n",
       "7   [myxxxcollection, com, v, js, jih, bpd, com, d...   \n",
       "8                                 [super, info, docs]   \n",
       "9   [horizonsgallery, com, js, bin, ssl, id, www, ...   \n",
       "10  [phlebolog, com, ua, libraries, joomla, result...   \n",
       "11  [docs, google, com, spreadsheet, viewform, for...   \n",
       "12  [www, coincoele, com, br, Scripts, smiles, pt,...   \n",
       "13  [www, henkdeinumboomkwekerij, nl, language, pd...   \n",
       "14  [perfectsolutionofall, net, wp, content, theme...   \n",
       "\n",
       "                                         text_stemmed  \n",
       "0   [nobel, it, ffb, d, dca, cce, f, login, skype,...  \n",
       "1   [www, dghjdgf, com, paypal, co, uk, cycgi, bin...  \n",
       "2   [serviciosbi, com, paypal, cgi, bin, get, into...  \n",
       "3   [mail, printakid, com, www, onlin, americanexp...  \n",
       "4   [thewhiskeydreg, com, wp, content, theme, wide...  \n",
       "5           [smilesvoegol, servebb, org, voegol, php]  \n",
       "6   [premierpaymentprocess, com, includ, boleto, v...  \n",
       "7   [myxxxcollect, com, v, js, jih, bpd, com, do, ...  \n",
       "8                                  [super, info, doc]  \n",
       "9   [horizonsgalleri, com, js, bin, ssl, id, www, ...  \n",
       "10  [phlebolog, com, ua, librari, joomla, result, ...  \n",
       "11  [doc, googl, com, spreadsheet, viewform, formk...  \n",
       "12  [www, coincoel, com, br, script, smile, pt, br...  \n",
       "13  [www, henkdeinumboomkwekerij, nl, languag, pdf...  \n",
       "14  [perfectsolutionofal, net, wp, content, theme,...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c31186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# joining all the list elements without any commas \n",
    "phish_data['text_sent'] = phish_data['text_stemmed'].map(lambda l: ' '.join(l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e48d6943",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[nobell, it, ffb, d, dca, cce, f, login, SkyPe...</td>\n",
       "      <td>[nobel, it, ffb, d, dca, cce, f, login, skype,...</td>\n",
       "      <td>nobel it ffb d dca cce f login skype com en cg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "      <td>www dghjdgf com paypal co uk cycgi bin webscrc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>bad</td>\n",
       "      <td>[serviciosbys, com, paypal, cgi, bin, get, int...</td>\n",
       "      <td>[serviciosbi, com, paypal, cgi, bin, get, into...</td>\n",
       "      <td>serviciosbi com paypal cgi bin get into herf s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>mail.printakid.com/www.online.americanexpress....</td>\n",
       "      <td>bad</td>\n",
       "      <td>[mail, printakid, com, www, online, americanex...</td>\n",
       "      <td>[mail, printakid, com, www, onlin, americanexp...</td>\n",
       "      <td>mail printakid com www onlin americanexpress c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>thewhiskeydregs.com/wp-content/themes/widescre...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[thewhiskeydregs, com, wp, content, themes, wi...</td>\n",
       "      <td>[thewhiskeydreg, com, wp, content, theme, wide...</td>\n",
       "      <td>thewhiskeydreg com wp content theme widescreen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>smilesvoegol.servebbs.org/voegol.php</td>\n",
       "      <td>bad</td>\n",
       "      <td>[smilesvoegol, servebbs, org, voegol, php]</td>\n",
       "      <td>[smilesvoegol, servebb, org, voegol, php]</td>\n",
       "      <td>smilesvoegol servebb org voegol php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>premierpaymentprocessing.com/includes/boleto-2...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[premierpaymentprocessing, com, includes, bole...</td>\n",
       "      <td>[premierpaymentprocess, com, includ, boleto, v...</td>\n",
       "      <td>premierpaymentprocess com includ boleto via php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>myxxxcollection.com/v1/js/jih321/bpd.com.do/do...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[myxxxcollection, com, v, js, jih, bpd, com, d...</td>\n",
       "      <td>[myxxxcollect, com, v, js, jih, bpd, com, do, ...</td>\n",
       "      <td>myxxxcollect com v js jih bpd com do do l popu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>super1000.info/docs</td>\n",
       "      <td>bad</td>\n",
       "      <td>[super, info, docs]</td>\n",
       "      <td>[super, info, doc]</td>\n",
       "      <td>super info doc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>horizonsgallery.com/js/bin/ssl1/_id/www.paypal...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[horizonsgallery, com, js, bin, ssl, id, www, ...</td>\n",
       "      <td>[horizonsgalleri, com, js, bin, ssl, id, www, ...</td>\n",
       "      <td>horizonsgalleri com js bin ssl id www paypal c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>phlebolog.com.ua/libraries/joomla/results.php</td>\n",
       "      <td>bad</td>\n",
       "      <td>[phlebolog, com, ua, libraries, joomla, result...</td>\n",
       "      <td>[phlebolog, com, ua, librari, joomla, result, ...</td>\n",
       "      <td>phlebolog com ua librari joomla result php</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>docs.google.com/spreadsheet/viewform?formkey=d...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[docs, google, com, spreadsheet, viewform, for...</td>\n",
       "      <td>[doc, googl, com, spreadsheet, viewform, formk...</td>\n",
       "      <td>doc googl com spreadsheet viewform formkey de ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>www.coincoele.com.br/Scripts/smiles/?pt-br/Pag...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, coincoele, com, br, Scripts, smiles, pt,...</td>\n",
       "      <td>[www, coincoel, com, br, script, smile, pt, br...</td>\n",
       "      <td>www coincoel com br script smile pt br pagina ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>www.henkdeinumboomkwekerij.nl/language/pdf_fon...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, henkdeinumboomkwekerij, nl, language, pd...</td>\n",
       "      <td>[www, henkdeinumboomkwekerij, nl, languag, pdf...</td>\n",
       "      <td>www henkdeinumboomkwekerij nl languag pdf font...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>perfectsolutionofall.net/wp-content/themes/twe...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[perfectsolutionofall, net, wp, content, theme...</td>\n",
       "      <td>[perfectsolutionofal, net, wp, content, theme,...</td>\n",
       "      <td>perfectsolutionofal net wp content theme twent...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  URL Label  \\\n",
       "0   nobell.it/70ffb52d079109dca5664cce6f317373782/...   bad   \n",
       "1   www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...   bad   \n",
       "2   serviciosbys.com/paypal.cgi.bin.get-into.herf....   bad   \n",
       "3   mail.printakid.com/www.online.americanexpress....   bad   \n",
       "4   thewhiskeydregs.com/wp-content/themes/widescre...   bad   \n",
       "5                smilesvoegol.servebbs.org/voegol.php   bad   \n",
       "6   premierpaymentprocessing.com/includes/boleto-2...   bad   \n",
       "7   myxxxcollection.com/v1/js/jih321/bpd.com.do/do...   bad   \n",
       "8                                 super1000.info/docs   bad   \n",
       "9   horizonsgallery.com/js/bin/ssl1/_id/www.paypal...   bad   \n",
       "10      phlebolog.com.ua/libraries/joomla/results.php   bad   \n",
       "11  docs.google.com/spreadsheet/viewform?formkey=d...   bad   \n",
       "12  www.coincoele.com.br/Scripts/smiles/?pt-br/Pag...   bad   \n",
       "13  www.henkdeinumboomkwekerij.nl/language/pdf_fon...   bad   \n",
       "14  perfectsolutionofall.net/wp-content/themes/twe...   bad   \n",
       "\n",
       "                                       text_tokenized  \\\n",
       "0   [nobell, it, ffb, d, dca, cce, f, login, SkyPe...   \n",
       "1   [www, dghjdgf, com, paypal, co, uk, cycgi, bin...   \n",
       "2   [serviciosbys, com, paypal, cgi, bin, get, int...   \n",
       "3   [mail, printakid, com, www, online, americanex...   \n",
       "4   [thewhiskeydregs, com, wp, content, themes, wi...   \n",
       "5          [smilesvoegol, servebbs, org, voegol, php]   \n",
       "6   [premierpaymentprocessing, com, includes, bole...   \n",
       "7   [myxxxcollection, com, v, js, jih, bpd, com, d...   \n",
       "8                                 [super, info, docs]   \n",
       "9   [horizonsgallery, com, js, bin, ssl, id, www, ...   \n",
       "10  [phlebolog, com, ua, libraries, joomla, result...   \n",
       "11  [docs, google, com, spreadsheet, viewform, for...   \n",
       "12  [www, coincoele, com, br, Scripts, smiles, pt,...   \n",
       "13  [www, henkdeinumboomkwekerij, nl, language, pd...   \n",
       "14  [perfectsolutionofall, net, wp, content, theme...   \n",
       "\n",
       "                                         text_stemmed  \\\n",
       "0   [nobel, it, ffb, d, dca, cce, f, login, skype,...   \n",
       "1   [www, dghjdgf, com, paypal, co, uk, cycgi, bin...   \n",
       "2   [serviciosbi, com, paypal, cgi, bin, get, into...   \n",
       "3   [mail, printakid, com, www, onlin, americanexp...   \n",
       "4   [thewhiskeydreg, com, wp, content, theme, wide...   \n",
       "5           [smilesvoegol, servebb, org, voegol, php]   \n",
       "6   [premierpaymentprocess, com, includ, boleto, v...   \n",
       "7   [myxxxcollect, com, v, js, jih, bpd, com, do, ...   \n",
       "8                                  [super, info, doc]   \n",
       "9   [horizonsgalleri, com, js, bin, ssl, id, www, ...   \n",
       "10  [phlebolog, com, ua, librari, joomla, result, ...   \n",
       "11  [doc, googl, com, spreadsheet, viewform, formk...   \n",
       "12  [www, coincoel, com, br, script, smile, pt, br...   \n",
       "13  [www, henkdeinumboomkwekerij, nl, languag, pdf...   \n",
       "14  [perfectsolutionofal, net, wp, content, theme,...   \n",
       "\n",
       "                                            text_sent  \n",
       "0   nobel it ffb d dca cce f login skype com en cg...  \n",
       "1   www dghjdgf com paypal co uk cycgi bin webscrc...  \n",
       "2   serviciosbi com paypal cgi bin get into herf s...  \n",
       "3   mail printakid com www onlin americanexpress c...  \n",
       "4   thewhiskeydreg com wp content theme widescreen...  \n",
       "5                 smilesvoegol servebb org voegol php  \n",
       "6     premierpaymentprocess com includ boleto via php  \n",
       "7   myxxxcollect com v js jih bpd com do do l popu...  \n",
       "8                                      super info doc  \n",
       "9   horizonsgalleri com js bin ssl id www paypal c...  \n",
       "10         phlebolog com ua librari joomla result php  \n",
       "11  doc googl com spreadsheet viewform formkey de ...  \n",
       "12  www coincoel com br script smile pt br pagina ...  \n",
       "13  www henkdeinumboomkwekerij nl languag pdf font...  \n",
       "14  perfectsolutionofal net wp content theme twent...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_data.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa962e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA CLEANING\n",
    "# noise = [\"[]\"]\n",
    "# df = pd.read_csv('cleaned_data.csv', na_values = noise )\n",
    "# df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e387113",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "phish_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404179c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058f1eb7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()))\n",
    "browser = webdriver.Chrome(r\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a66d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_urls = ['https://ezee.com/cell-phones/','https://ezee.com/login.php?from=account.php%3Faction%3D'] #here i take phishing sites \n",
    "links_with_text = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ccf41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for url in list_urls:\n",
    "    driver.get(url)\n",
    "    soup = BeautifulSoup(driver.page_source,\"html.parser\")\n",
    "    for line in soup.find_all('a' or 'link'):\n",
    "        href = line.get('href')\n",
    "        links_with_text.append([url, href])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6495414a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links_with_text, columns=[\"from\", \"to\"])\n",
    "# DataFrame is 2D datastructure in pandas.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22f27982",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('dataframe.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a895b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# network graph using networkx \n",
    "GA = nx.from_pandas_edgelist(df, source=\"from\", target=\"to\")\n",
    "nx.draw(GA, with_labels=False) # draws the internal hyperlink redirection network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad34a89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer # Vectorizes the tokens into a matrix \n",
    "\n",
    "cv = CountVectorizer() # converts the tokenized-stemmed words into a sparse matrix in a numerical format w.r.t frequency of word, as machine can't understand words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "eb4b04f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Label</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_stemmed</th>\n",
       "      <th>text_sent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>nobell.it/70ffb52d079109dca5664cce6f317373782/...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[nobell, it, ffb, d, dca, cce, f, login, SkyPe...</td>\n",
       "      <td>[nobel, it, ffb, d, dca, cce, f, login, skype,...</td>\n",
       "      <td>nobel it ffb d dca cce f login skype com en cg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...</td>\n",
       "      <td>bad</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "      <td>[www, dghjdgf, com, paypal, co, uk, cycgi, bin...</td>\n",
       "      <td>www dghjdgf com paypal co uk cycgi bin webscrc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>serviciosbys.com/paypal.cgi.bin.get-into.herf....</td>\n",
       "      <td>bad</td>\n",
       "      <td>[serviciosbys, com, paypal, cgi, bin, get, int...</td>\n",
       "      <td>[serviciosbi, com, paypal, cgi, bin, get, into...</td>\n",
       "      <td>serviciosbi com paypal cgi bin get into herf s...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL Label  \\\n",
       "0  nobell.it/70ffb52d079109dca5664cce6f317373782/...   bad   \n",
       "1  www.dghjdgf.com/paypal.co.uk/cycgi-bin/webscrc...   bad   \n",
       "2  serviciosbys.com/paypal.cgi.bin.get-into.herf....   bad   \n",
       "\n",
       "                                      text_tokenized  \\\n",
       "0  [nobell, it, ffb, d, dca, cce, f, login, SkyPe...   \n",
       "1  [www, dghjdgf, com, paypal, co, uk, cycgi, bin...   \n",
       "2  [serviciosbys, com, paypal, cgi, bin, get, int...   \n",
       "\n",
       "                                        text_stemmed  \\\n",
       "0  [nobel, it, ffb, d, dca, cce, f, login, skype,...   \n",
       "1  [www, dghjdgf, com, paypal, co, uk, cycgi, bin...   \n",
       "2  [serviciosbi, com, paypal, cgi, bin, get, into...   \n",
       "\n",
       "                                           text_sent  \n",
       "0  nobel it ffb d dca cce f login skype com en cg...  \n",
       "1  www dghjdgf com paypal co uk cycgi bin webscrc...  \n",
       "2  serviciosbi com paypal cgi bin get into herf s...  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phish_data[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0529d2ef",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(phish_data.text_sent) # converts the tokens into algo-understandable numerical format by calculating different statistical terms like mean, standard deviation etc...,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "06875ec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<549346x350837 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 3676066 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature[:5]\n",
    "feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0235e67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)\n",
    "# trainX & testX are compressed sparse matrixes\n",
    "# trainY & testY are splitted datasets \n",
    "# unable to store 1.4TiB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321fe0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 549346 ) # Increasing the max_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca9967c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the lr model with training split data of X & Y \n",
    "lr.fit(trainX,trainY)\n",
    "# ConvergenceWarning: lbfgs failed to converge (status=1): STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
    "# Increase the number of iterations (max_iter) or scale the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b411a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# scaler = preprocessing.StandardScaler().fit(trainY) \n",
    "# scaled = scaler.transform(trainX) \n",
    "\n",
    "# cant enter strings to be scaled \n",
    "# sparse matrices can't be chosen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bd6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ab205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.score(testX,testY) \n",
    "# accuracy improved from 0.960 --> 0.9664 on increasing the max_iter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b97d19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing the performance in dictionary \n",
    "score = {}\n",
    "score['LogReg'] = np.round (lr.score(testX,testY) , 3 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf50cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['LogReg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9889eec7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Training Accuracy :',lr.score(trainX,trainY))\n",
    "print('Testing Accuracy :',lr.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(lr.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(lr.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sb.heatmap(con_mat, annot = True,fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6b66b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# comparing the true labels i.e. true dataset with the predicted dataset and prepares the report of quality of prediction.\n",
    "print(classification_report(testY, lr.predict(testX)))\n",
    "# predictions will be done from sparse matrices. \n",
    "# Accuracy - out of all the good and bad predictions how many of them are acutally true\n",
    "# Precision - out of all good/(+ve) predictions done how many of them are true +ve. similiarly for bad/(-ve)\n",
    "# recall - it simply says how truthful the model is \n",
    "# recall - no. of predicted good / no.of actual real good  \n",
    "# let's say no of +ve s are 6 but predicted are only 4 to be good \n",
    "# recall = 4 / 6 = 0.67\n",
    "# f1 score = harmonic mean of precision & recall\n",
    "# hm = 2*(h1 * h2 ) / ( h1 + h2 ) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaab38ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## MULTINOMIAL NAIVE BAYES ALGORITHM \n",
    "mnb = MultinomialNB()\n",
    "mnb.fit(trainX,trainY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784ebb8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.score(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd74772",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb.score(testX,testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d51ddd5e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiNB\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mround(mnb\u001b[38;5;241m.\u001b[39mscore(testX,testY), \u001b[38;5;241m2\u001b[39m )\n\u001b[0;32m      2\u001b[0m score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultiNB\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "score['multiNB'] = np.round(mnb.score(testX,testY), 2 )\n",
    "score['multiNB']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9565d3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy :',mnb.score(trainX,trainY))\n",
    "print('Testing Accuracy :',mnb.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(mnb.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(mnb.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sb.heatmap(con_mat, annot = True,fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23b8ac",
   "metadata": {},
   "source": [
    "## LOGISTIC REGRESSION PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "af4dc145",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(max_iter = 549347)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48513843",
   "metadata": {},
   "source": [
    "### COUNT VECTORIZER PIPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91e74ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline = make_pipeline(cv(tokenizer.tokenize) , lr ) \n",
    "# arguments - ( preprocessing_techniques , model(i.e. estimator))\n",
    "lr_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize), lr)\n",
    "# lr_pipeline = make_pipeline(TfidfVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2bd5695f",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "positional argument follows keyword argument (2554803405.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [21], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    lr_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,SnowballStemmer(),stop_words='english'), lr)\u001b[0m\n\u001b[1;37m                                                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m positional argument follows keyword argument\n"
     ]
    }
   ],
   "source": [
    "# INCLUDING SNOWBALLSTEMMER IN TRANSFORMERS \n",
    "lr_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,SnowballStemmer(),stop_words='english'), lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3894bc",
   "metadata": {},
   "source": [
    "### TF-IDF PIPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "63ca7d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_pipeline = make_pipeline(TfidfTransformer(), lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2df15017",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = cv.fit_transform(phish_data.text_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84e40d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training the model with feature extractions and labelled data \n",
    "trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38d7e231",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [52], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlr_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m lr_pipeline\u001b[38;5;241m.\u001b[39mscore(testX,testY) \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:390\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    364\u001b[0m \u001b[38;5;124;03m\"\"\"Fit the model.\u001b[39;00m\n\u001b[0;32m    365\u001b[0m \n\u001b[0;32m    366\u001b[0m \u001b[38;5;124;03mFit all the transformers one after the other and transform the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Pipeline with fitted steps.\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    389\u001b[0m fit_params_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_fit_params(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 390\u001b[0m Xt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps)\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:348\u001b[0m, in \u001b[0;36mPipeline._fit\u001b[1;34m(self, X, y, **fit_params_steps)\u001b[0m\n\u001b[0;32m    346\u001b[0m     cloned_transformer \u001b[38;5;241m=\u001b[39m clone(transformer)\n\u001b[0;32m    347\u001b[0m \u001b[38;5;66;03m# Fit or load from cache the current transformer\u001b[39;00m\n\u001b[1;32m--> 348\u001b[0m X, fitted_transformer \u001b[38;5;241m=\u001b[39m fit_transform_one_cached(\n\u001b[0;32m    349\u001b[0m     cloned_transformer,\n\u001b[0;32m    350\u001b[0m     X,\n\u001b[0;32m    351\u001b[0m     y,\n\u001b[0;32m    352\u001b[0m     \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    353\u001b[0m     message_clsname\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    354\u001b[0m     message\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_message(step_idx),\n\u001b[0;32m    355\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params_steps[name],\n\u001b[0;32m    356\u001b[0m )\n\u001b[0;32m    357\u001b[0m \u001b[38;5;66;03m# Replace the transformer of the step with the fitted\u001b[39;00m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;66;03m# transformer. This is necessary when loading the transformer\u001b[39;00m\n\u001b[0;32m    359\u001b[0m \u001b[38;5;66;03m# from the cache.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[step_idx] \u001b[38;5;241m=\u001b[39m (name, fitted_transformer)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\memory.py:349\u001b[0m, in \u001b[0;36mNotMemorizedFunc.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:893\u001b[0m, in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    891\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _print_elapsed_time(message_clsname, message):\n\u001b[0;32m    892\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(transformer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit_transform\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 893\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit_transform(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    894\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    895\u001b[0m         res \u001b[38;5;241m=\u001b[39m transformer\u001b[38;5;241m.\u001b[39mfit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\u001b[38;5;241m.\u001b[39mtransform(X)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1201\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m raw_documents:\n\u001b[0;32m   1200\u001b[0m     feature_counter \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m-> 1201\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m \u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1202\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1203\u001b[0m             feature_idx \u001b[38;5;241m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:113\u001b[0m, in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m preprocessor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m         doc \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m         doc \u001b[38;5;241m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:71\u001b[0m, in \u001b[0;36m_preprocess\u001b[1;34m(doc, accent_function, lower)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;124;03m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;124;03mapply to a document.\u001b[39;00m\n\u001b[0;32m     54\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;124;03m    preprocessed string\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lower:\n\u001b[1;32m---> 71\u001b[0m     doc \u001b[38;5;241m=\u001b[39m \u001b[43mdoc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accent_function \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     73\u001b[0m     doc \u001b[38;5;241m=\u001b[39m accent_function(doc)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\scipy\\sparse\\base.py:687\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m    685\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgetnnz()\n\u001b[0;32m    686\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 687\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(attr \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "lr_pipeline.fit(trainX,trainY)\n",
    "lr_pipeline.score(testX,testY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a071b873",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'https://www.codewithrandom.com/2022/08/24/search-icon-inside-input-html-css/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [46], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mlr_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://www.codewithrandom.com/2022/08/24/search-icon-inside-input-html-css/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    467\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 469\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1660\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1660\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1664\u001b[0m         X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:746\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    744\u001b[0m         array \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mastype(dtype, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m\"\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    745\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 746\u001b[0m         array \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    748\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    749\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    750\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'https://www.codewithrandom.com/2022/08/24/search-icon-inside-input-html-css/'"
     ]
    }
   ],
   "source": [
    "lr_pipeline.predict(\"https://www.codewithrandom.com/2022/08/24/search-icon-inside-input-html-css/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9ec3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['LogReg_tfid_feature_pipeline'] = lr_pipeline.score(testX,testY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd6fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy :',lr_pipeline.score(trainX,trainY))\n",
    "print('Testing Accuracy :',lr_pipeline.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(lr_pipeline.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(lr_pipeline.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sb.heatmap(con_mat, annot = True,fmt='d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c95d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(lr_pipeline, open('lr_model.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1244190",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = pickle.load(open('lr_model.pkl','rb'))\n",
    "result = lr_model.score(testX, testY) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d5411c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.round(result , 2 ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48faf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['LogReg_pipeline'] = lr_model.score(testX, testY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c77dd8e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [30], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m linear_model\n\u001b[0;32m      3\u001b[0m reg \u001b[38;5;241m=\u001b[39m linear_model\u001b[38;5;241m.\u001b[39mBayesianRidge()\n\u001b[1;32m----> 4\u001b[0m \u001b[43mreg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtrainY\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m reg\u001b[38;5;241m.\u001b[39mscore(testX,testY)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_bayes.py:240\u001b[0m, in \u001b[0;36mBayesianRidge.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    234\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    235\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_iter should be greater than or equal to 1. Got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    236\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter\n\u001b[0;32m    237\u001b[0m         )\n\u001b[0;32m    238\u001b[0m     )\n\u001b[1;32m--> 240\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sample_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    243\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mdtype)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:581\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    579\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 581\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:964\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    962\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my cannot be None\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 964\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    965\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    966\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    967\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    968\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    969\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    970\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    979\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric)\n\u001b[0;32m    981\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:720\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(array):\n\u001b[0;32m    719\u001b[0m     _ensure_no_complex_data(array)\n\u001b[1;32m--> 720\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_sparse_format\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[43m        \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    727\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    728\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    729\u001b[0m     \u001b[38;5;66;03m# If np.array(..) gives ComplexWarning, then we convert the warning\u001b[39;00m\n\u001b[0;32m    730\u001b[0m     \u001b[38;5;66;03m# to an error. This is needed because specifying a non complex\u001b[39;00m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# dtype to the function converts complex to real dtype,\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;66;03m# thereby passing the test made in the lines following the scope\u001b[39;00m\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;66;03m# of warnings context manager.\u001b[39;00m\n\u001b[0;32m    734\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings():\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:440\u001b[0m, in \u001b[0;36m_ensure_sparse_format\u001b[1;34m(spmatrix, accept_sparse, dtype, copy, force_all_finite, accept_large_sparse)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_large_sparse(spmatrix, accept_large_sparse)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accept_sparse \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    441\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA sparse matrix was passed, but dense \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata is required. Use X.toarray() to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    443\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert to a dense numpy array.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    444\u001b[0m     )\n\u001b[0;32m    445\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(accept_sparse, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m    446\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(accept_sparse) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: A sparse matrix was passed, but dense data is required. Use X.toarray() to convert to a dense numpy array."
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.BayesianRidge()\n",
    "reg.fit(trainX,trainY)\n",
    "reg.score(testX,testY)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec4dbf",
   "metadata": {},
   "source": [
    "## MULTINOMIAL-NB PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0d281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COUNTVECTORIZER TRANSFORMER WITHOUT FEATURES EXTRACTED\n",
    "mnb_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words = 'english'),mnb)\n",
    "trainX, testX, trainY, testY = train_test_split(phish_data.URL, phish_data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9450079",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipeline.fit(trainX, trainY) \n",
    "mnb_pipeline.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94b3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['multiNB_pipeline'] = mnb_pipeline.score(testX, testY) \n",
    "pickle.dump(mnb_pipeline, open('mnb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8d8c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDFTRANSFORMER WITH FEATURES EXTRACTED\n",
    "mnb_pipeline = make_pipeline(TfidfTransformer(),mnb)\n",
    "trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95fbdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_pipeline.fit(trainX, trainY) \n",
    "mnb_pipeline.score(testX, testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ad833",
   "metadata": {},
   "outputs": [],
   "source": [
    "score['multiNB_tfid_feature_pipeline'] = mnb_pipeline.score(testX, testY) \n",
    "pickle.dump(mnb_pipeline, open('mnb_model.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f7a65a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFIDFVECTORIZER TRANSFORMER WITH FEATURES EXTRACTED\n",
    "# mnb_pipeline = make_pipeline(TfidfVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words = 'english'),mnb)\n",
    "# trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)\n",
    "# VECTORIZERS NEED LOWER BOUND WITH THE FEATURE EXTRACTION DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77290cf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnb_model = pickle.load(open('mnb_model.pkl', 'rb'))\n",
    "mnb_model.score(testX,testY) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769f1751",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training Accuracy :',mnb_pipeline.score(trainX,trainY))\n",
    "print('Testing Accuracy :',mnb_pipeline.score(testX,testY))\n",
    "con_mat = pd.DataFrame(confusion_matrix(mnb_pipeline.predict(testX), testY),\n",
    "            columns = ['Predicted:Bad', 'Predicted:Good'],\n",
    "            index = ['Actual:Bad', 'Actual:Good'])\n",
    "\n",
    "print('\\nCLASSIFICATION REPORT\\n')\n",
    "print(classification_report(mnb_pipeline.predict(testX), testY,\n",
    "                            target_names =['Bad','Good']))\n",
    "\n",
    "print('\\nCONFUSION MATRIX')\n",
    "plt.figure(figsize= (6,4))\n",
    "sb.heatmap(con_mat, annot = True,fmt='d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d78619b",
   "metadata": {},
   "source": [
    "# OTHER PIPELINE MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9105debd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb \n",
    "xgb_classifier = xgb.XGBClassifier() \n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_classifier = SGDClassifier() \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(max_iter = 110359) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier() \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC() \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier() \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_classifier = MultinomialNB() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01743dae",
   "metadata": {},
   "source": [
    "### COUNT VECTORIZER PIPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d26ca265",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), xgb_classifier)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8bf83507",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), ada_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1153c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = make_pipeline(CountVectorizer(tokenizer = RegexpTokenizer(r'[A-Za-z]+').tokenize,stop_words='english'), sgd_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5f8b81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer # Vectorizes the tokens into a matrix \n",
    "\n",
    "cv = CountVectorizer() # converts the tokenized-stemmed words into a sparse matrix in a numerical format w.r.t frequency of word, as machine can't understand words\n",
    "feature = cv.fit_transform(phish_data.text_sent) # converts the tokens into algo-understandable numerical format by calculating different statistical terms like mean, standard deviation etc...,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "615f609d",
   "metadata": {},
   "source": [
    "### TF-IDF PIPES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4b97b53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pipeline = make_pipeline(TfidfTransformer(), xgb_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f64ed0a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pipeline = make_pipeline(TfidfTransformer(), ada_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0ed5a58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipeline = make_pipeline(TfidfTransformer(), sgd_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72535647",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer # Vectorizes the tokens into a matrix \n",
    "\n",
    "cv = CountVectorizer()\n",
    "feature = cv.fit_transform(phish_data.text_sent)\n",
    "# training the model with feature extractions and labelled data \n",
    "trainX, testX, trainY, testY = train_test_split(feature, phish_data.Label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5f6a9ace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "78444     good\n",
       "178700    good\n",
       "424825    good\n",
       "106682     bad\n",
       "315430    good\n",
       "106491     bad\n",
       "263791    good\n",
       "170055    good\n",
       "122187     bad\n",
       "198075    good\n",
       "Name: Label, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainY.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "356e1293",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "trainY = le.fit_transform(trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3f2e8249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "for i in range( 0 , 9) : \n",
    "    print( trainY[i] ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "80b7c516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidftransformer', TfidfTransformer()),\n",
       "                ('xgbclassifier',\n",
       "                 XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "                               colsample_bylevel=1, colsample_bynode=1,\n",
       "                               colsample_bytree=1, early_stopping_rounds=None,\n",
       "                               enable_categorical=False, eval_metric=None,\n",
       "                               feature_types=None, gamma=0, gpu_id=-1,\n",
       "                               grow_policy='depthwise', importance_type=None,\n",
       "                               interaction_constraints='',\n",
       "                               learning_rate=0.300000012, max_bin=256,\n",
       "                               max_cat_threshold=64, max_cat_to_onehot=4,\n",
       "                               max_delta_step=0, max_depth=6, max_leaves=0,\n",
       "                               min_child_weight=1, missing=nan,\n",
       "                               monotone_constraints='()', n_estimators=100,\n",
       "                               n_jobs=0, num_parallel_tree=1, predictor='auto',\n",
       "                               random_state=0, ...))])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipeline.fit( trainX , trainY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e95211bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "testY = le.fit_transform(testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01830856",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9132426076002825"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_pipeline.score( testX, testY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ad441969",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[1. 0. 0. ... 1. 1. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mxgb_pipeline\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestY\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py:113\u001b[0m, in \u001b[0;36m_AvailableIfDescriptor.__get__.<locals>.<lambda>\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m attr_err\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;66;03m# lambda, but not partial, allows help() to work with update_wrapper\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfn(obj, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# noqa\u001b[39;00m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    116\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py:469\u001b[0m, in \u001b[0;36mPipeline.predict\u001b[1;34m(self, X, **predict_params)\u001b[0m\n\u001b[0;32m    467\u001b[0m Xt \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iter(with_final\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 469\u001b[0m     Xt \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpredict(Xt, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpredict_params)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1660\u001b[0m, in \u001b[0;36mTfidfTransformer.transform\u001b[1;34m(self, X, copy)\u001b[0m\n\u001b[0;32m   1643\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1644\u001b[0m     \u001b[38;5;124;03m\"\"\"Transform a count matrix to a tf or tf-idf representation.\u001b[39;00m\n\u001b[0;32m   1645\u001b[0m \n\u001b[0;32m   1646\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[38;5;124;03m        Tf-idf-weighted document-term matrix.\u001b[39;00m\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1660\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1661\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1663\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m sp\u001b[38;5;241m.\u001b[39missparse(X):\n\u001b[0;32m   1664\u001b[0m         X \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mcsr_matrix(X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:566\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    564\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation should be done on X, y or both.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m no_val_y:\n\u001b[1;32m--> 566\u001b[0m     X \u001b[38;5;241m=\u001b[39m check_array(X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    567\u001b[0m     out \u001b[38;5;241m=\u001b[39m X\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_y:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:769\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    767\u001b[0m     \u001b[38;5;66;03m# If input is 1D raise error\u001b[39;00m\n\u001b[0;32m    768\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m array\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 769\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    770\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected 2D array, got 1D array instead:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124marray=\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    771\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReshape your data either using array.reshape(-1, 1) if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    772\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour data has a single feature or array.reshape(1, -1) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    773\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mif it contains a single sample.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m    774\u001b[0m         )\n\u001b[0;32m    776\u001b[0m \u001b[38;5;66;03m# make sure we actually converted to numeric:\u001b[39;00m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype_numeric \u001b[38;5;129;01mand\u001b[39;00m array\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mkind \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOUSV\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[1. 0. 0. ... 1. 1. 0.].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "xgb_pipeline.predict( testY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59b3b59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(xgb_pipeline, open('xgb_pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "986b4ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('tfidftransformer', TfidfTransformer()),\n",
       "                ('sgdclassifier', SGDClassifier())])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipeline.fit( trainX , trainY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9cb71c59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9121649664693419"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_pipeline.score( testX, testY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "01c5d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(sgd_pipeline, open('sgd_pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e9738a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ada_pipeline.fit( trainX , trainY ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf37145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(ada_pipeline, open('ada_pipeline.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d89374e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945066d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b61ae1b4",
   "metadata": {},
   "source": [
    "## BERT TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96061ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    import torch\n",
    "    import transformers as ppb # pytorch transformers\n",
    "    \n",
    "    \n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    \n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    from sklearn.naive_bayes import MultinomialNB\n",
    "    \n",
    "    import warnings\n",
    "\n",
    "    import swifter\n",
    "    import tqdm\n",
    "    tqdm.pandas()\n",
    "\n",
    "    warnings.filterwarnings('ignore')\n",
    "except Exception  as e: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7d5fbcbc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = {}\n",
    "df = pd.read_csv('phishing_site_urls.csv')\n",
    "# df = df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96811a04",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3621\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3620\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:136\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:163\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5198\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5206\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 1",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      2\u001b[0m Y \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      3\u001b[0m encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3505\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3504\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3505\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3507\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3623\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3621\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3622\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3623\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3624\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3625\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3626\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3627\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3628\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 1"
     ]
    }
   ],
   "source": [
    "X = df[1]\n",
    "Y = df[2]\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(Y)\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7cc6da11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTokenizer(object):\n",
    "\n",
    "    def __init__(self, text=[]):\n",
    "        self.text = text\n",
    "\n",
    "        # For DistilBERT:\n",
    "        self.model_class, self.tokenizer_class, self.pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "        # Load pretrained model/tokenizer\n",
    "        self.tokenizer = self.tokenizer_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "        self.model = self.model_class.from_pretrained(self.pretrained_weights)\n",
    "\n",
    "    def get(self):\n",
    "\n",
    "        df = pd.DataFrame(data={\"text\":self.text})\n",
    "        tokenized = df[\"text\"].swifter.apply((lambda x: self.tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "        max_len = 0\n",
    "        for i in tokenized.values:\n",
    "            if len(i) > max_len:\n",
    "                max_len = len(i)\n",
    "\n",
    "        padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "        attention_mask = np.where(padded != 0, 1, 0)\n",
    "        input_ids = torch.tensor(padded)\n",
    "        attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "        with torch.no_grad(): last_hidden_states = self.model(input_ids, attention_mask=attention_mask)\n",
    "        \n",
    "        features = last_hidden_states[0][:, 0, :].numpy()\n",
    "\n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8a204d9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m _instance \u001b[38;5;241m=\u001b[39mBertTokenizer(text\u001b[38;5;241m=\u001b[39m\u001b[43mx_train\u001b[49m)\n\u001b[0;32m      2\u001b[0m tokens \u001b[38;5;241m=\u001b[39m _instance\u001b[38;5;241m.\u001b[39mget()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "_instance =BertTokenizer(text=x_train)\n",
    "tokens = _instance.get()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04af641a",
   "metadata": {},
   "source": [
    "# MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2552b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(tokens, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f545de4",
   "metadata": {},
   "source": [
    "# TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa6e936",
   "metadata": {},
   "outputs": [],
   "source": [
    "_instance =BertTokenizer(text=x_test)\n",
    "tokensTest = _instance.get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3201eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = lr_clf.predict(tokensTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489c04aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(predicted == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9742fdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35738f86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a41fd51a",
   "metadata": {},
   "source": [
    "# Performance evaluation of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d4ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = pd.DataFrame.from_dict(score,orient = 'index',columns=['Accuracy'])\n",
    "sb.set_style('darkgrid')\n",
    "sb.barplot(acc.index,acc.Accuracy)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2985d2d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pyngrok nest_asyncio fastapi uvicorn loguru\n",
    "import uvicorn\n",
    "from fastapi import FastAPI\n",
    "import joblib,os\n",
    "from fastapi import FastAPI\n",
    "from pydantic import BaseModel\n",
    "from loguru import logger\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "#pkl\n",
    "phish_model = open('lr_model.pkl','rb')\n",
    "phish_model_ls = joblib.load(phish_model)\n",
    "\n",
    "# ML Aspect\n",
    "@app.get('/predict/{feature}')\n",
    "async def predict(features):\n",
    "\tX_predict = []\n",
    "\tX_predict.append(str(features))\n",
    "\ty_Predict = phish_model_ls.predict(X_predict)\n",
    "\tif y_Predict == 'bad':\n",
    "\t\tresult = \"This is a Phishing Site\"\n",
    "\telse:\n",
    "\t\tresult = \"This is not a Phishing Site\"\n",
    "\n",
    "\treturn (features, result)\n",
    "if __name__ == '__main__':\n",
    "\tuvicorn.run(app,host=\"127.0.0.1\",port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36337cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7387bc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
