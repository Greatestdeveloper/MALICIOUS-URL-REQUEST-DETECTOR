{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5217641d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Machine-Learning-Web-Application-Firewall-and-Dataset-master\n"
     ]
    }
   ],
   "source": [
    "cd D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Machine-Learning-Web-Application-Firewall-and-Dataset-master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ded15f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "# from sklearn.neighbors.nearest_centroid import NearestCentroid\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "import sklearn.gaussian_process.kernels as kernels\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from scipy.stats import expon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "891db115",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def get1Grams(payload_obj):\n",
    "    '''Divides a string into 1-grams\n",
    "    \n",
    "    Example: input - payload: \"<script>\"\n",
    "             output- [\"<\",\"s\",\"c\",\"r\",\"i\",\"p\",\"t\",\">\"]\n",
    "    '''\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = []\n",
    "    for i in range(0,len(payload)-1):\n",
    "        ngrams.append(payload[i:i+1])\n",
    "    return ngrams\n",
    "\n",
    "tfidf_vectorizer_1grams = TfidfVectorizer(tokenizer=get1Grams)\n",
    "count_vectorizer_1grams = CountVectorizer(min_df=1, tokenizer=get1Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4401692c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the payload --> 1gram tokenization --> insert as an attribute in csv --> vectorize it & set it as X --> train_test_split\n",
    "# pipeline the model with all the nlp transformers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e3e6dfd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<',\n",
       " 'B',\n",
       " 'R',\n",
       " ' ',\n",
       " 'S',\n",
       " 'I',\n",
       " 'Z',\n",
       " 'E',\n",
       " '=',\n",
       " '\"',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '&',\n",
       " '{',\n",
       " 'a',\n",
       " 'l',\n",
       " 'e',\n",
       " 'r',\n",
       " 't',\n",
       " '(',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " '1',\n",
       " ')',\n",
       " '}',\n",
       " '\"']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " get1Grams('<BR SIZE=\"&&&&&&&&&&&{alert(111111111111111)}\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4d21167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get2Grams(payload_obj):\n",
    "    '''Divides a string into 2-grams\n",
    "    \n",
    "    Example: input - payload: \"<script>\"\n",
    "             output- [\"<s\",\"sc\",\"cr\",\"ri\",\"ip\",\"pt\",\"t>\"]\n",
    "    '''\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = []\n",
    "    for i in range(0,len(payload)-2):\n",
    "        ngrams.append(payload[i:i+2])\n",
    "    return ngrams\n",
    "\n",
    "tfidf_vectorizer_2grams = TfidfVectorizer(tokenizer=get2Grams)\n",
    "count_vectorizer_2grams = CountVectorizer(min_df=1, tokenizer=get2Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2641a516",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get3Grams(payload_obj):\n",
    "    '''Divides a string into 3-grams\n",
    "    \n",
    "    Example: input - payload: \"<script>\"\n",
    "             output- [\"<sc\",\"scr\",\"cri\",\"rip\",\"ipt\",\"pt>\"]\n",
    "    '''\n",
    "    payload = str(payload_obj)\n",
    "    ngrams = []\n",
    "    for i in range(0,len(payload)-3):\n",
    "        ngrams.append(payload[i:i+3])\n",
    "    return ngrams\n",
    "\n",
    "tfidf_vectorizer_3grams = TfidfVectorizer(tokenizer=get3Grams)\n",
    "count_vectorizer_3grams = CountVectorizer(min_df=1, tokenizer=get3Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a3c47e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer # Vectorizes the tokens into a matrix \n",
    "\n",
    "cv = CountVectorizer() \n",
    "count_vectorizer_1grams = CountVectorizer(min_df=1, tokenizer=get1Grams)\n",
    "count_vectorizer_2grams = CountVectorizer(min_df=1, tokenizer=get2Grams)\n",
    "count_vectorizer_2grams = CountVectorizer(min_df=1, tokenizer=get3Grams)\n",
    "\n",
    "tfidf_vectorizer_1grams = TfidfVectorizer(tokenizer=get1Grams)\n",
    "tfidf_vectorizer_2grams = TfidfVectorizer(tokenizer=get2Grams)\n",
    "tfidf_vectorizer_3grams = TfidfVectorizer(tokenizer=get3Grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6587ade3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e73a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7f1955",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc33794",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ae7acc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "xgb_classifier = xgb.XGBClassifier()\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "ada_classifier = AdaBoostClassifier()\n",
    "\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "sgd_classifier = SGDClassifier() \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr_classifier = LogisticRegression(max_iter = 110359) \n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_classifier = RandomForestClassifier() \n",
    "\n",
    "from sklearn.svm import SVC\n",
    "svm_classifier = SVC() \n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt_classifier = DecisionTreeClassifier() \n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "mnb_classifier = MultinomialNB() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "14841bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "payload           0\n",
       "is_malicious      0\n",
       "injection_type    0\n",
       "length            0\n",
       "non-printable     0\n",
       "punctuation       0\n",
       "min-byte          0\n",
       "max-byte          0\n",
       "mean-byte         0\n",
       "std-byte          0\n",
       "distinct-bytes    0\n",
       "sql-keywords      0\n",
       "js-keywords       0\n",
       "tokens_1          0\n",
       "tokens_2          0\n",
       "tokens_3          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads = {} \n",
    "payloads = pd.read_csv(\"data/payloadss.csv\",index_col='index')\n",
    "# TypeError: expected string or bytes-like object - drop the null values \n",
    "payloads.dropna(inplace=True)\n",
    "payloads.isnull().sum()\n",
    "# payloads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b496d3b9",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35d3ba74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer # Tokenizes the url to get the required words\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[A-Za-z]+') # tokenizes the given url based on the string argument r'[a-zA-Z]\\w+\\'?\\w*'\n",
    "payloads['tokens_1'] = payloads.payload.map(lambda t: tokenizer.tokenize(t)) # doing with all rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04c4fab2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0                               [P]\n",
       "1                        [shirting]\n",
       "2                  [kw, alert, XSS]\n",
       "3                         [obeying]\n",
       "4                       [dictating]\n",
       "5                         [lafleur]\n",
       "6                       [capturers]\n",
       "7                          [nca, z]\n",
       "8                      [autocratic]\n",
       "9             [grocery, warehouses]\n",
       "10                        [Danciel]\n",
       "11                        [Koressa]\n",
       "12    [bowie, showeb, handling, bi]\n",
       "13                    [Auque, Cote]\n",
       "14                       [broadest]\n",
       "15                [Xaubet, Sorolla]\n",
       "16                     [vocational]\n",
       "17                          [Yemen]\n",
       "18                               []\n",
       "19                [BR, SIZE, alert]\n",
       "Name: tokens_1, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads.tokens_1.head(20) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "411d1e69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0                    P\n",
       "1             shirting\n",
       "2           kwalertXSS\n",
       "3              obeying\n",
       "4            dictating\n",
       "5              lafleur\n",
       "6            capturers\n",
       "7                 ncaz\n",
       "8           autocratic\n",
       "9    grocerywarehouses\n",
       "Name: join_tokens_1, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads['join_tokens_1'] = payloads.tokens_1.map( lambda t : ''.join(t))\n",
    "payloads.join_tokens_1.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bb7a3a39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# running it twice to get space between \n",
    "payloads['c_tokens_1'] = payloads.join_tokens_1.map( lambda t : get1Grams(t))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6a302e98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0                                                    []\n",
       "1                                 [s, h, i, r, t, i, n]\n",
       "2                           [k, w, a, l, e, r, t, X, S]\n",
       "3                                    [o, b, e, y, i, n]\n",
       "4                              [d, i, c, t, a, t, i, n]\n",
       "5                                    [l, a, f, l, e, u]\n",
       "6                              [c, a, p, t, u, r, e, r]\n",
       "7                                             [n, c, a]\n",
       "8                           [a, u, t, o, c, r, a, t, i]\n",
       "9      [g, r, o, c, e, r, y, w, a, r, e, h, o, u, s, e]\n",
       "10                                   [D, a, n, c, i, e]\n",
       "11                                   [K, o, r, e, s, s]\n",
       "12    [b, o, w, i, e, s, h, o, w, e, b, h, a, n, d, ...\n",
       "13                             [A, u, q, u, e, C, o, t]\n",
       "14                                [b, r, o, a, d, e, s]\n",
       "Name: c_tokens_1, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads.c_tokens_1.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f9cf8dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "index\n",
       "0                                   \n",
       "1                      s h i r t i n\n",
       "2                  k w a l e r t X S\n",
       "3                        o b e y i n\n",
       "4                    d i c t a t i n\n",
       "5                        l a f l e u\n",
       "6                    c a p t u r e r\n",
       "7                              n c a\n",
       "8                  a u t o c r a t i\n",
       "9    g r o c e r y w a r e h o u s e\n",
       "Name: 1g_string, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "payloads['1g_string'] = payloads.c_tokens_1.map( lambda t : ' '.join(t))\n",
    "payloads['1g_string'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2a51415f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[' ', ' ', ' ', ' ', ' ', ' ', 'h', 'i', 'i', 'n', 'r', 's', 't']\n"
     ]
    }
   ],
   "source": [
    "# TRYING TO CONVERT ABOVE STRINGS s h i r t i n INTO ['s h i r t i n']\n",
    "print( sorted(payloads['1g_string'][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6185a92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CANT ALLOCATE MEMORY --> CAN'T CONVERT STRING - FLOAT --> INSUFFICIENT FEATURES \n",
    "\n",
    "# ValueError: could not convert string to float: '&kw=%27;alert%28%27XSS%27%29;//'\n",
    "# ValueError: X has 18 features, but SVC is expecting 172 features as input.\n",
    "\n",
    "# AttributeError: 'list' object has no attribute 'lower'\n",
    "# occurs when we call the lower() method on a list instead of a string\n",
    "\n",
    "# numericals are included in the dataset and the transform will be done only after converting the words into their lower case\n",
    "# as numericals don't have the lower case it is returning the above error \n",
    "\n",
    "# ValueError: empty vocabulary; perhaps the documents only contain stop words\n",
    "# Triggered because of string s h i r t i n is input to transform but it needs ['s h i r t i n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8608be68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "empty vocabulary; perhaps the documents only contain stop words",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mCountVectorizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmin_df\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpayloads\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m1g_string\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m Y \u001b[38;5;241m=\u001b[39m payloads[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_malicious\u001b[39m\u001b[38;5;124m'\u001b[39m] \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1330\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1322\u001b[0m             warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1323\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUpper case characters found in\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1324\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m vocabulary while \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1325\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is True. These entries will not\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1326\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m be matched with any documents\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1327\u001b[0m             )\n\u001b[0;32m   1328\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m-> 1330\u001b[0m vocabulary, X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_count_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_documents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfixed_vocabulary_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbinary:\n\u001b[0;32m   1333\u001b[0m     X\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mfill(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:1220\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1218\u001b[0m     vocabulary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(vocabulary)\n\u001b[0;32m   1219\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m vocabulary:\n\u001b[1;32m-> 1220\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1221\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1222\u001b[0m         )\n\u001b[0;32m   1224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indptr[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:  \u001b[38;5;66;03m# = 2**31 - 1\u001b[39;00m\n\u001b[0;32m   1225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _IS_32BIT:\n",
      "\u001b[1;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"
     ]
    }
   ],
   "source": [
    "# ['s h i r t i n'] WILL GET TRANSFORMED \n",
    "X = CountVectorizer(min_df = 1).fit_transform(payloads['1g_string'])\n",
    "Y = payloads['is_malicious'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b607ce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX, testX, trainY, testY = train_test_split(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "10ea715d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testX.to_csv('testX.csv')\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for df in path1:\n",
    "#     df = pd.read_excel(df)  \n",
    "#     data.append(df)\n",
    "\n",
    "# data.to_csv(\"H:\\\\test1.csv\", index = False)\n",
    "\n",
    "# data = []\n",
    "\n",
    "# for df in testX :\n",
    "#     df = pd.read_list(df) \n",
    "#     data.append(df) \n",
    "    \n",
    "# data.to_csv('testX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a73d732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGDClassifier()"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_classifier.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45616e09",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sgd_classifier.predict( testX )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8d6530bd",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 6)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 13)\t2\n",
      "  (0, 0)\t1\n",
      "  (0, 14)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 10)\t2\n",
      "  (0, 7)\t1\n",
      "  (0, 1)\t2\n",
      "  (0, 2)\t11\n",
      "  (0, 17)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 5)\t15\n",
      "  (0, 4)\t1\n",
      "  (0, 18)\t1\n"
     ]
    }
   ],
   "source": [
    "a = ['<BR SIZE=\"&&&&&&&&&&&{alert(111111111111111)}\">']\n",
    "aa = count_vectorizer_1grams.fit_transform(a)\n",
    "print( aa ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "87c53288",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 19 features, but SGDClassifier is expecting 172 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43msgd_classifier\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43maa\u001b[49m\u001b[43m \u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:425\u001b[0m, in \u001b[0;36mLinearClassifierMixin.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    411\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[0;32m    412\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    413\u001b[0m \u001b[38;5;124;03m    Predict class labels for samples in X.\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    423\u001b[0m \u001b[38;5;124;03m        Vector containing the class labels for each sample.\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 425\u001b[0m     scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    426\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(scores\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    427\u001b[0m         indices \u001b[38;5;241m=\u001b[39m (scores \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_base.py:407\u001b[0m, in \u001b[0;36mLinearClassifierMixin.decision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    388\u001b[0m \u001b[38;5;124;03mPredict confidence scores for samples.\u001b[39;00m\n\u001b[0;32m    389\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;124;03m    this class would be predicted.\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    405\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m--> 407\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    408\u001b[0m scores \u001b[38;5;241m=\u001b[39m safe_sparse_dot(X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_\u001b[38;5;241m.\u001b[39mT, dense_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_\n\u001b[0;32m    409\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mravel() \u001b[38;5;28;01mif\u001b[39;00m scores\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m scores\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m features as input.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 19 features, but SGDClassifier is expecting 172 features as input."
     ]
    }
   ],
   "source": [
    "sgd_classifier.predict( aa ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9ddb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_classifier.fit(trainX,trainY)\n",
    "xgb_classifier.fit(trainX,trainY)\n",
    "ada_classifier.fit(trainX,trainY)\n",
    "sgd_classifier.fit(trainX,trainY)\n",
    "lr_classifier.fit(trainX,trainY)\n",
    "svm_classifier.fit(trainX,trainY)\n",
    "dt_classifier.fit(trainX,trainY)\n",
    "mnb_classifier.fit(trainX,trainY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592e000b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27873a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5658c2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe3b437",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897f995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86342cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9de90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26c736b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eaa932b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fcdb86",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a648f4e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465b2135",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e987df60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b888eae1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d1fd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dd8791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8974f645",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee58031",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff195127",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ceb5b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b48255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc838eff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2a566b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3f257",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "372ef941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Machine-Learning-Web-Application-Firewall-and-Dataset-master\\gram_models\n"
     ]
    }
   ],
   "source": [
    "cd D:\\1MYFILES\\ML_NGFW\\FIREWALLS\\-Machine-Learning-Web-Application-Firewall-and-Dataset-master\\gram_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a40f2b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = pickle.load(open(\"xgb_classifier_cv_1.pkl\", \"rb\")) \n",
    "ada = pickle.load(open(\"ada_classifier_cv_1.pkl\", \"rb\"))\n",
    "sgd = pickle.load(open(\"sgd_classifier_cv_1.pkl\", \"rb\"))\n",
    "lr = pickle.load(open(\"lr_classifier_cv_1.pkl\", \"rb\"))\n",
    "rf = pickle.load(open(\"rf_classifier_cv_1.pkl\", \"rb\"))\n",
    "svm = pickle.load(open(\"svm_classifier_cv_1.pkl\", \"rb\"))\n",
    "dt = pickle.load(open(\"dt_classifier_cv_1.pkl\", \"rb\"))\n",
    "mnb = pickle.load(open(\"mnb_classifier_cv_1.pkl\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "33fb4fa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'apply'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [66], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# url = ['<BR SIZE=\"&{alert(1)}\">']\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m url \u001b[38;5;241m=\u001b[39m count_vectorizer_1grams\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m<BR SIZE=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m&\u001b[39;49m\u001b[38;5;124;43m{\u001b[39;49m\u001b[38;5;124;43malert(1)}\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m>\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m(\u001b[38;5;28;01mlambda\u001b[39;00m x: np\u001b[38;5;241m.\u001b[39mstr_(x)))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'apply'"
     ]
    }
   ],
   "source": [
    "# url = ['<BR SIZE=\"&{alert(1)}\">']\n",
    "url = count_vectorizer_1grams.fit_transform(['<BR SIZE=\"&{alert(1)}\">'].apply(lambda x: np.str_(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c5846244",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Feature shape mismatch, expected: 54265, got 19",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [59], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mxgb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m \n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1551\u001b[0m, in \u001b[0;36mXGBClassifier.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\n\u001b[0;32m   1542\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1543\u001b[0m     X: ArrayLike,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     iteration_range: Optional[Tuple[\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mint\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1549\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m   1550\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity):\n\u001b[1;32m-> 1551\u001b[0m         class_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[43m            \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1553\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1554\u001b[0m \u001b[43m            \u001b[49m\u001b[43mntree_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mntree_limit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1555\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1556\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1557\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1558\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1559\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m output_margin:\n\u001b[0;32m   1560\u001b[0m             \u001b[38;5;66;03m# If output_margin is active, simply return the scores\u001b[39;00m\n\u001b[0;32m   1561\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m class_probs\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1140\u001b[0m, in \u001b[0;36mXGBModel.predict\u001b[1;34m(self, X, output_margin, ntree_limit, validate_features, base_margin, iteration_range)\u001b[0m\n\u001b[0;32m   1138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_can_use_inplace_predict():\n\u001b[0;32m   1139\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1140\u001b[0m         predts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_booster\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace_predict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1141\u001b[0m \u001b[43m            \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1142\u001b[0m \u001b[43m            \u001b[49m\u001b[43miteration_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43miteration_range\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1143\u001b[0m \u001b[43m            \u001b[49m\u001b[43mpredict_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmargin\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43moutput_margin\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mvalue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1144\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmissing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmissing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1145\u001b[0m \u001b[43m            \u001b[49m\u001b[43mbase_margin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbase_margin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1146\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalidate_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1147\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1148\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m _is_cupy_array(predts):\n\u001b[0;32m   1149\u001b[0m             \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcupy\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=import-error\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\xgboost\\core.py:2268\u001b[0m, in \u001b[0;36mBooster.inplace_predict\u001b[1;34m(self, data, iteration_range, predict_type, missing, validate_features, base_margin, strict_shape)\u001b[0m\n\u001b[0;32m   2264\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m   2265\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`shape` attribute is required when `validate_features` is True.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2266\u001b[0m         )\n\u001b[0;32m   2267\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features() \u001b[38;5;241m!=\u001b[39m data\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m-> 2268\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   2269\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature shape mismatch, expected: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_features()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2270\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2271\u001b[0m         )\n\u001b[0;32m   2273\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m   2274\u001b[0m     _array_interface,\n\u001b[0;32m   2275\u001b[0m     _is_cudf_df,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2278\u001b[0m     _transform_pandas_df,\n\u001b[0;32m   2279\u001b[0m )\n\u001b[0;32m   2281\u001b[0m enable_categorical \u001b[38;5;241m=\u001b[39m _has_categorical(\u001b[38;5;28mself\u001b[39m, data)\n",
      "\u001b[1;31mValueError\u001b[0m: Feature shape mismatch, expected: 54265, got 19"
     ]
    }
   ],
   "source": [
    "xgb.predict(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3dee96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16fe5ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
